---
title: "Homework 4"
author: '***Yuwen(Suyi)Wu 5:45-7:45***'
date: "4/25/2019"
output: word_document
---
```{r,echo=FALSE,include=FALSE}
require(tidyverse)
require(gridExtra)
require(plotrix)
require(knitr)
require(ggplot2)
```
#Part One

###Question 1
```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=14, fig.height=10}
# Import Data
dating <- read_delim("SpeedDating.csv", col_names=TRUE, delim=",")

# Table Fill
both_want <- length(which(dating$DecisionM == dating$DecisionF & dating$DecisionM == 1))
both_not <- length(which(dating$DecisionM == dating$DecisionF & dating$DecisionM == 0))
Female1Male0 <- length(which(dating$DecisionF == 1 & dating$DecisionM == 0))
Female0Male1 <- length(which(dating$DecisionF == 0 & dating$DecisionM == 1))

# Plot Table
decision <- data.frame('Decision of Female (No)' = c(both_not,Female1Male0), 'Decision of Female (Yes)' = c(Female0Male1,both_want),row.names = c("Decision of Male (No)","Decision of Male (Yes)"))

# Calculate Percentage
both_want_percent <- both_want/nrow(dating)

kable(decision)
both_want_percent
```


As the result, under calculation, there is 22.83% of dates ended with both people wanting a second date


###Question 2 
```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=20, fig.height=16}
# Add a new column
second.date <- rep(0,nrow(dating))
dating <- data.frame(dating, second.date)
dating$second.date[which(dating$DecisionM == dating$DecisionF & dating$DecisionM == 1)] <- 1

# setting pchs
pchs <- rep(NA,nrow(dating))
pchs[which(dating[,"second.date"] == 1)] <- 19
pchs[which(dating[,"second.date"] == 0)] <- 4

# setting colors
color.setting <- rep(NA,nrow(dating))
color.setting [which(dating[,"second.date"] == 1)] <- "hotpink"
color.setting [which(dating[,"second.date"] == 0)] <- "royalblue"

# Except Race Data
numb <- seq(from = 3 , to = 21 ,by = 2) 
numb <- numb[-4]

# Scatter Plots 

theme.info <- theme(plot.title = element_text(size=30, hjust=0.5),
                    axis.title = element_text(size=30),
                    axis.text = element_text(size=30),
                    legend.title = element_text(color = "black", size = 30),
                    legend.text = element_text(color = "black", size = 30))

par(mfrow=c(3,3))
colnames <- dimnames(dating)[[2]]


# col.vector <- c("second date Yes"="hotpink","second date No"="royalblue")

for (i in numb) {
  plot(as.data.frame(dating)[,i], as.data.frame(dating)[,i+1], pch=pchs,col = color.setting,
       main=paste("Scatterplot of" , colnames[i], "vs.",colnames[i+1]),xlab=colnames[i],
       ylab=colnames[i+1],cex.main=1.0, cex.lab=0.8, cex.axis=0.8)+theme.info
}

legend("bottomright", legend=c("2nd date", "no 2nd date"), bty="n", col=c("hotpink", "royalblue"), pch= c(19,4), cex=1.4)


```

(1) Like: Like indicator means how much you like this person.The hotpink dots cluster at top right of the scatter plot, indicating the higher “Like” score of M and F gives to the partner, a greater chance they will have a second date. However, there is still some conditions that people give high score to the partner but they don't have second date. From the scatter plot, it seems people tend to give high score to the partner, all dots are clustered at top right, thus it may be the reason for people don't have second date even high "Like" score of both.

(2) PartnerYes: PartnerYes indicator means how probable do you think it is that this person will say "yes" to you. The hotpink dots are clustered at top right of the scatter plot, indicating the higher “PartnerYes” score of M and F gives to the partner, a greater chance they will have a second date. 

(3) Age: From the scatter plot, hotpink dots appears more close to the “Y=X” line, which means people are more willing to have a second date when they are at a similar age.

(4) Attractive: Attractive is an indicator means attractiveness rate of partners on a scale of 1 to 10, hotpink dots appears in the topright region of the plot, where both gender gives the rate of “Attractive”(to the other person) higher than 5. Therefore, the higher both scores provided, the higher probability a second date will occur. 

(5) Sincere: Sincere is an indicator for partner to rate sincerity of partners on a scale of 1 to 10. From the scatter plot, most candidates provide scores between 6-10 and in most of time, people tends to show sincere in speed dating events to increase a second date chance. Thus, partner tends to provide high scores for sincerity rate. In scatter plot of Sincere F and Sincere M, it is not obvious that there is an relationship between sincere rate and possibility of a second date.

(6) Intelligent : Intelligent is an indicator for partner to rate intelligence of another one in speed dating. From the scatter plot, most dots are on the up half picture which means almost all female provide score from 4 to 10. However, male provide scores are evenly distributed from 1 to 10. We can see that when both give each other with similar scores of "intelligence", the higher chance they will have a second date.

(7)	Fun: Fun is an indicator for partner to rate how fun of the other on the scale of 1 to 10. We can see that if both feel the other is fun, which means high score or hotpink dots on the right top part, they will have high probability to have a second date.

(8)	Ambitious: Ambitious is an indicator for parnter to rate ambitious of the other on the scale of 1 to 10. From the scatter plot, it seems that male with high ambitious rate judged by female would be have higher probability to gain a second date.

(9)	SharedInterest: SharedInterest is an indicator for partner to rate whether he or she shared similar interest with the other. Most of the second date cases occur when both gender give a similar high score to their partner, which means they both regard each other has most similar interests with them. 



###Question 3

```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=20, fig.height=16}
# Check Range
summary(dating)
# Adjust Range
dating$PartnerYesM[which(dating$PartnerYesM==0)]<-1
dating$FunM[which(dating$FunM==0)]<-1
dating$SharedInterestsF[which(dating$SharedInterestsF==0)]<-1
dating$SharedInterestsM[which(dating$SharedInterestsM==0)]<-1

# Check Missing Data 
print(length(which(is.na(dating$RaceF)==TRUE)))
print(length(which(is.na(dating$RaceM)==TRUE)))

# Missing Data

Missing_Data <- matrix(c(2,4,4,4,3,5,3,2,5,3,8,3,6,6,17,10,27,30), byrow = TRUE, ncol = 2)
rownames(Missing_Data) <- c("Like","PartnerYes","Age","Attractive","Sincere","Intelligent",
                 "Fun","Ambitious","SharedInterest")
colnames(Missing_Data) <- c("NA number (from response by Male)", "NA number (from response by Female)")
print(Missing_Data)

```

Since some data from 1 to 10 rather than instructed 0 to 10, these data should be adjused from data of 1 to 0.For the data of 0, it could be 10 mistakenly written as 0 as well.

From above summary, except the Decision M, Decision F and Second Date, all other variables are exist missing data, which represented as NAs in summary. There are 2 missing data for Like M, 4 missing data for LikeF, 4 missing data for PartnerYesM, 4 missing data for PartnerYesF, 3 missing data for Age M, 5 missing data for Age F, 3 missing data for Attractive M, 2 missing data for Attractive F, 5 missing data for Sincere M, 3 missing data for Sincere F, 8 missing data for Intelligent M, 3 missing data for Intelligent F, 6 missing data for FunM, 6 missing data for Fun F, 17 missing data for Ambitious M, 10 missing data for Ambitious F, 27 missing data for Shared Interests M, 30 missing data for SharedInterestsF.

Especially for Race data, for Race F, 4 missing data and for Race M, 2 missing data.

###Question 4

```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=25, fig.height=16}
dating_check <- dating[!complete.cases(dating[,c("RaceF","RaceM")]),]
# checking missing data of dating raceF and raceM 
print(dating_check)
dating_full <- dating
race_category_M <- dating_full %>% distinct(RaceM,.keep_all = FALSE)
race_category_F <- dating_full %>% distinct(RaceF,.keep_all = FALSE)
print(race_category_M)
print(race_category_F)

temp <- tibble(dating_full$RaceM,dating_full$RaceF)
mosaicplot(table(temp), 
           main="Mosaic Plot with Female and Male Race", 
           xlab="RaceM", ylab="RaceF", 
           las=TRUE, cex.axis=1.2,color = c("#663A44", "#5F7880", "#CAA78D", 	"#2F4F4F","grey","gold"))

```

In this dataset, we have races of Caucasian, Asian, Latino, Black and other.

For Race F, 4 missing data and for Race M, 2 missing data. 
I would like to keep them in the dataset for Mosaic Plot 
(1) We are not sure whether we will use Race factor in the model, otherwise, missing data does not matter;
(2) In the further logistic regression, missing data will be automatically remove; 

From the mosaic plot:
(1) Caucasian and Asian are the largest two portions of race in this dataset;
(2) There is no date match group in this case, with two people's races are combination of 
(a.) Black male + Black female, (b.) Other races male+Black female.

###Question 5
```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=25, fig.height=16}

# Logit Regression Model

logit.1 <- glm(formula=second.date~LikeM+LikeF+PartnerYesM+PartnerYesF+AttractiveM+AttractiveF
               +SincereF+SincereM+IntelligentF+IntelligentM+FunF+FunM+SharedInterestsF+SharedInterestsM
               +AmbitiousF+AmbitiousM,family = binomial(link="logit"),data = dating)
summary(logit.1)

# Remove the factor of Sincere M and SharedInterestsF

logit.2 <- glm(formula=second.date~LikeM+LikeF+PartnerYesM+PartnerYesF+AttractiveM+AttractiveF
               +IntelligentF+IntelligentM+FunF+FunM+SharedInterestsM+SincereF
               +AmbitiousF+AmbitiousM,family = binomial(link="logit"),data = dating)
summary(logit.2)

# Remove the factor of Intelligent F and SharedInterestsM

logit.3 <- glm(formula=second.date~LikeM+LikeF+PartnerYesM+PartnerYesF+AttractiveM+AttractiveF
               +IntelligentM+FunF+FunM+SincereF+AmbitiousF+AmbitiousM,family =
                 binomial(link="logit"),data = dating)
summary(logit.3)

# Remove the factor of IntelligentM and LikeF

logit.4 <- glm(formula=second.date~LikeM+PartnerYesM+PartnerYesF+AttractiveM+AttractiveF
               +FunF+FunM+SincereF+AmbitiousF+AmbitiousM,family = binomial(link="logit"),
               data = dating)
summary(logit.4)

# Remove the factor of SincereF and AmbitiousM

logit.5 <- glm(formula=second.date~LikeM+PartnerYesM+PartnerYesF+AttractiveM+AttractiveF
               +FunF+FunM+AmbitiousF,family = binomial(link="logit"),data = dating)
summary(logit.5)

# Remove the factor of AttractiveM and FunM

logit.6 <- glm(formula=second.date~LikeM+PartnerYesM+PartnerYesF+AttractiveF
               +FunF+AmbitiousF,family = binomial(link="logit"),data = dating)
summary(logit.6)

```

```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=25, fig.height=16}
# Using Forward Method for Logit Regression
require(leaps)
logit.fw <- regsubsets(second.date ~ LikeM+LikeF+PartnerYesM+PartnerYesF+AttractiveM+AttractiveF
               +SincereF+SincereM+IntelligentF+IntelligentM+FunF+FunM+SharedInterestsF+SharedInterestsM
               +AmbitiousF+AmbitiousM, data=dating, method="forward", nvmax=15)
summary(logit.fw)
a<-data.frame("regression"=paste("trial",c(1:15),sep = "_"),
                "RMSE"=round(sqrt(summary(logit.fw)$rss),digits = 4),
               "adj.R^2"=round(summary(logit.fw)$adjr2, digits = 4),
                "C.P"=round(summary(logit.fw)$cp, digits = 4),
                "BIC"=round(summary(logit.fw)$bic, digits = 4), stringsAsFactors = FALSE)
a
```

````{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=25, fig.height=16}
# Using Backward Method for Logit Regression
logit.bw <- regsubsets(second.date ~ LikeM+LikeF+PartnerYesM+PartnerYesF+AttractiveM+AttractiveF
               +SincereF+SincereM+IntelligentF+IntelligentM+FunF+FunM+SharedInterestsF+SharedInterestsM
               +AmbitiousF+AmbitiousM, data=dating, method="backward", nvmax=15)
summary(logit.bw)
b<-data.frame("regression"=paste("trial",c(1:15),sep = "_"),
                "RMSE"=round(sqrt(summary(logit.fw)$rss),digits = 4),
               "adj.R^2"=round(summary(logit.fw)$adjr2, digits = 4),
                "C.P"=round(summary(logit.fw)$cp, digits = 4),
                "BIC"=round(summary(logit.fw)$bic, digits = 4), stringsAsFactors = FALSE)
b
```

From above Backward and Forward Regression Method, we can see that model with lowest BIC is tril 4 which is factor with LikeM/ FunF / PartnerYesM / PartnerYesF. But the model with highest Adjusted R^2 is tirl 6 with factor of AttractiveF/LikeM/ FunF/ PartnerYesM / PartnerYesF/AmbitiousM.

```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=25, fig.height=16}
logit.7 <- glm(formula=second.date~LikeM+PartnerYesM+PartnerYesF+AttractiveF
               +FunF+AmbitiousM,family = binomial(link="logit"),data = dating)
summary(logit.7)
# Since this factor AmbitiousM & FunF is not significance, we should get rid of it 
logit.8 <- glm(formula=second.date~LikeM+PartnerYesM+PartnerYesF+AttractiveF
               ,family = binomial(link="logit"),data = dating)
summary(logit.8)
summary(logit.6)

# From above 2 logit model, which all factors are significant, we compared the AIC and other criterions 

AIC <- c(summary(logit.6)$aic, summary(logit.8)$aic)
dev.null <- c(summary(logit.6)$null.deviance, summary(logit.8)$null.deviance)
dev <- c(summary(logit.6)$deviance, summary(logit.8)$deviance)
def.null <- c(summary(logit.6)$df.null, summary(logit.8)$df.null)
criterion <- data.frame("AIC"=AIC,"Null Deviance"=dev.null,"Deviance"=dev,
                        "Null d.f"=def.null )
rownames(criterion) <- c("best by Original","best by Backward/Forward")
library(knitr)
kable(t(criterion))

 

```

From Above Table, we can see that logit.8 has higher AIC but some variables at logit.6 will be unsignificant when alpha been set at 0.01

```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=25, fig.height=16}
final.model<- logit.8
summary(final.model)
```
####### Assumptions Checking #######
```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=25, fig.height=20}


#### Checking Outlier #####

dating.q5 <- dating[,c("LikeM","AttractiveF","PartnerYesM","PartnerYesF")]
plot(dating.q5,pch=20,col = "deeppink")
print(round(range(cooks.distance(final.model)),digits = 4))

#### Checking Multicollinearity #####
library(usdm)
vif(dating.q5[complete.cases(dating.q5),])

##### Check Sample Size ######
print(nrow(dating.q5))

##### Computing P-value #####
pchisq(summary(final.model)$null.deviance-summary(final.model)$deviance,
       df=summary(final.model)$df.null - summary(final.model)$df.residual, lower.tail=FALSE)


```

1) explanatory variables are measured without error: No measurement error in X variables, assumption satisfied.

2) model is correctly specified (no extraneous variables, all important variables included, etc.): model cannot be known as correctly specified!  There may be variables that weren’t collected which are relevant; perhaps a transformation may have been the “correct” model, etc., assumption unsatisfied.

3) outcomes not completely linearly separable: Every candidates give one specific result of second.date, therefore observations can be determined completely linearly separable. And we can do glm() in R, which also means this assumption is satisfied.

4) no outliers: The range of Cook's distance is (0.000, 0.0069). And no observations that has Cook's distance larger than critical value.

5) observations are independent: Data collected from individals attending speed dating randomly, assumption satisfied.

6) collinearity/multicollinearity: VIFs are close to 1, multicollinearity assumption satisifed.

7) sample size, n: #rule of thumb: at least 10 observations for each outcome (0/1) per predictor in your model. 

Just looking at overall sample size is not enough because, in theory, 276 rows of data could have 170 observations with 0 as the outcome and only 6 with 1 as the outcome.  
You have 4 predictors, so you need 10*4=40 observations for each outcome and you have 205 and 63 observations for 0 and 1 outcomes.  Therefore, the sample size assumption is satisfied.  




##### Model Evaluation ######
(1) log-likelihood for overall model  
H0 : βTempF=0
Ha : βTempF̸=0
α = 0.05
```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=25, fig.height=20}
# test statistic:
pchisq(summary(final.model)$null.deviance - summary(final.model)$deviance, 
       df=summary(final.model)$df.null - summary(final.model)$df.residual,
       lower.tail=FALSE)
```
The calculated p-value is 2.644361e-18, which much smaller than 0.05, we can reject H0 and thus, the remaining variables are all significant in the model.


(2) z-test for slopes
for each variable,
Ho: slope(beta) is equal to 0
H1: slope(beta) is not 0
```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=25, fig.height=20}
print(summary(final.model)$coefficient)
```
The p-value for each variables are all smller than 0.01, they are all significant to reject the null hypothesis.

To conclude, this model seems to be a good fit for the data.
And my final model is 
P(have a second date | LikeM, PartnerYesM, PartnerYesF, AttractiveF)=
exp^(-10.8811 + 0.4834LikeM + 0.3506PartnerYesM + 0.2799PartnerYesF + 0.3504AttractiveF)/
(1+exp^(-10.8811 + 0.4834LikeM + 0.3506PartnerYesM + 0.2799PartnerYesF + 0.3504AttractiveF))

###Question 6
```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=25, fig.height=20}
# Final Model Dataset
dating.q6 <- dating[complete.cases(dating[,c("LikeM","AttractiveF","PartnerYesM","PartnerYesF")]),]
# Table Fill 
both_want_fm <- length(which(dating.q6$DecisionF == 1 & dating$DecisionM == 1))
both_not_fm <- length(which(dating.q6$DecisionF ==0 & dating$DecisionM == 0))
Female1Male0_fm <- length(which(dating.q6$DecisionF == 1 & dating.q6$DecisionM == 0))
Female0Male1_fm <- length(which(dating.q6$DecisionF == 0 & dating.q6$DecisionM == 1))

# Plot Table 
decision_fm <- data.frame('Decision of Female (No)' = c(both_not_fm,Female1Male0_fm), 'Decision of Female (Yes)' = c(Female0Male1_fm,both_want_fm),row.names = c("Decision of Male (No)","Decision of Male (Yes)"))

print(decision_fm)

# Checking Sample Size
print(nrow(dating.q6))
Q6 <- rep(0, times=nrow(dating.q6))
Q6[which(dating.q6$DecisionF==1 & dating.q6$DecisionM==1)] <- 1
print(table(Q6))

```
The sample size is 268, and the number of explanatory variables in final model does not follow rule of thumb since both has second dating is 63 but the other group without second dating is 205.

###Question 7
```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=25, fig.height=20}
# all coefficient 
print(summary(final.model)$coefficient)
# all ranges 
print(summary(dating.q6[,c("second.date","LikeM","PartnerYesM","PartnerYesF","AttractiveF")])[c(1,6),])
# When all variables are Zero
print(exp(-10.8811)/(1+exp(-10.8811)))
# For LikeM Increase 
print(exp(exp(summary(final.model)$coefficient[2,1])/(1+exp(summary(final.model)$coefficient[2,1])))-1)
# For PartnerYesM Increase 
print(exp(exp(summary(final.model)$coefficient[3,1])/(1+exp(summary(final.model)$coefficient[3,1])))-1)
# For PartnerYesF Increase 
print(exp(exp(summary(final.model)$coefficient[4,1])/(1+exp(summary(final.model)$coefficient[4,1])))-1)
# For Attractive F Increase 
print(exp(exp(summary(final.model)$coefficient[5,1])/(1+exp(summary(final.model)$coefficient[5,1])))-1)

```

(1) Intercept
When all variables are zero, the probability that the two persons will have second date is exp(-10.8811)/(1+exp(-10.8811)), or 1.88*10^-5. 
However, the rating levels are range from 1 to 10. Therefore, the condition of "LikeM = AttractiveF = PartnerYesM = PartnerYesF = 0 " is impossible. Thus, the interpretation of intercept is meaningless.

(2) LikeM
When LikeM ranking score increases by 1,holding all other x’s fixed, the odds of having a second date increases by 85.62%.

(3) Partner Yes M
When PartnerYesM ranking score increases by 1, holding all other x’s fixed, the odds of having a second date increases by 79.81%.

(4) PartnerYesF
When PartnerYesF ranking score increases by 1, holding all other x’s fixed, the odds of having a second date increases by 76.74%.

(5) AttractiveF 
If AttractiveF ranking score increases by 1, holding all other x’s fixed, the odds of having a second date increases by 79.81%.

All the independent variables in the final model would increase the probability of a second date. It is consistant with my expectation.
In the final model, the variables such as LikeM or AttractiveF, the more attractive they score of their partners, the higher probability for the second date. Therefore, the final model could be considered reasonable.

###Question 8
```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=12, fig.height=10}
require(pROC)
dating.q8 <- dating.q6[,c("second.date","LikeM","AttractiveF","PartnerYesM","PartnerYesF")]
rownames(dating.q8) <- 1:nrow(dating.q8)
# plot ROC 
roc(response=dating.q8$second.date, predictor=final.model$fitted.values,
    plot=TRUE, las=TRUE, 	legacy.axes=TRUE, lwd=5,
    main="ROC Curve", cex.main=1.6, cex.axis=1.3, cex.lab=1.3,xlab = "1-Specificity\n (false positive rate)",ylab = "Sensitivity\n(true positive rate)")+theme.info
# get AUC 
print(auc(response=dating.q8$second.date, predictor=final.model$fitted.values))

```

The AUC of the ROC curve is 0.8602.
In this case, we want to decrease false positive rate, which is predicting a group will have a second dating but in fact they don't want to have a second date, and we want increase true negative(TN) predictions, which is predicting a group will have a second dating and they actually have.

```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=15, fig.height=10}
# save ROC curve into an object 
roc.info <- roc(response=dating.q8$second.date, predictor=final.model$fitted.values)
# sensitivity and specificity for the threshold with highest sensitivity + specificity
print(coords(roc.info, x="best", ret=c("threshold", "specificity", "sensitivity")))
# sensitivity and specificity for a wide range of thresholds
# use t() to transpose output from coords() for easier use
pi.range <- t(coords(roc.info, x="all", ret=c("threshold", "specificity", "sensitivity")))
dim(pi.range)
# plot sum of sensitivity and specificity against threshold
par(mfrow=c(1,2))
# plot ROC with best Threshold
roc(response=dating.q8$second.date, predictor=final.model$fitted.values,
    plot=TRUE, las=TRUE, lwd=3,	legacy.axes=TRUE, 
    main="ROC for Second Date Analysis", cex.main=1.3, cex.axis=1.1, cex.lab=1.1,xlab = "1-Specificity\n (false positive rate)",ylab = "Sensitivity\n(true positive rate)")+theme.info

# adding best sum of Threshold to ROC plot 
best <- as.data.frame(t(coords(roc.info, x="best", ret=c("threshold", "specificity", "sensitivity"))))
points(best$specificity, best$sensitivity, pch=19, col="firebrick")
legend("bottomright", legend=paste("AUC=", round(roc.info$auc, digits=3), sep=" "),
       pch=19, col="firebrick", bty="n", cex=1.9, y.intersp = 1.3)

# plot pi range 
plot(pi.range[2:243, "threshold"], pi.range[2:243, "sensitivity"] + pi.range[2:243, "specificity"], 
     type="l", las=TRUE, xlab=expression(paste("Threshold, ", pi^"*", sep="")), ylab="Sensitivity + Specificity", 
     main="Sensitivity + Specificity Against Threshold", cex.axis=1.1, cex.lab=1.1, 
     cex.main=1.3, lwd=2, xlim=c(0, 1))+theme.info

# adding best sum to plot 
points(best$threshold, best$specificity + best$sensitivity, pch=19, col="firebrick")
legend("topright", legend=paste("best threshold =", round(best$threshold, digits=4)),
                            pch=19, col="firebrick", bty="n", cex=1.9)

# compute accuracy 
temp <- dating.q8
rownames(temp) <- 1:nrow(temp)
temp <- data.frame(temp, "fitted.values"=round(final.model$fitted.values, digits=3))


actual.sec <- rep("second.date", times=nrow(temp))
actual.sec[temp$second.date == 0] <- "no second.date"

classify.best <- rep("second.date", times=nrow(temp))
classify.best[temp$fitted.values < coords(roc.info, x="best", ret="threshold")] <- "no second.date"

print(table(classify.best, actual.sec))

print(coords(roc.info, x="best", ret=c("threshold","accuracy", "specificity", "sensitivity")))
```

The threshold should be adjusted to best shreshold which is  0.1653049, in this case, the accuracy is 0.7238806, the specificity is 0.6634146 and the sensitivit is 0.9206349.

#Part Two
###Question 9 Code
```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=25, fig.height=20}
# Import Data
require(readxl)
kudzu_data<-read_excel("kudzu.xls")
# Response Variable 
kudzu_data$BMD
```
The response variable is BMD, which is bone mineral density.

###Question 10 
```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=25, fig.height=20}
# Check Factor 
print(table(kudzu_data$Treatment))
```
The two factors are HighDoes group and LowDose group.
The levels are HighDoes, LowDoes and Control.

###Question 11 
There are only 2 factors, there are 3 kinds of treatments.

###Question 12 
completely randomized design

###Question 13
```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=25, fig.height=20}
# summary statistics
Sample.Size <- "15"
# compute mean for each treatment group
mean.kudzu<-aggregate(kudzu_data$BMD, by=list(kudzu_data$Treatment), mean)

# compute standard deviation for each treatment group
sd.kudzu<-aggregate(kudzu_data$BMD, by=list(kudzu_data$Treatment), sd)

treatment.group <- data.frame("Sample Size"=Sample.Size,"Mean(in grams per square centimeter)"=mean.kudzu$x,"Standard Deviation(in grams per square centimeter)"=sd.kudzu$x )
rownames(treatment.group) <- c("Control","HighDose","LowDose")
colnames(treatment.group) <- c("Sample Size","Mean(in grams per square centimeter)","Standard Deviation(in grams per square centimeter)")
print(kable(t(treatment.group)))
```

###Question 14 
```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=12, fig.height=10}
# side-by-side boxplots with connecting means ######
theme.info <- theme(plot.title = element_text(size=16, hjust=0.5),
                    axis.title = element_text(size=14),
                    axis.text = element_text(size=14))

kudzu_data %>% 
  ggplot(aes(Treatment,BMD)) + 
  geom_boxplot() +
  stat_summary(fun.y=mean, geom="line", aes(group=1), lwd=2, col="cadetblue") +
  stat_summary(fun.y=mean, geom="point", pch=19, size=2, col="firebrick") +
  ggtitle("Boxplots of BMD by Treatment Group\nWith Connected Means") +
  labs(x="treatment group", 
       y="bone mineral density\n(in grams per square centimeter)") +
  theme.info                    
```

From above side-by-side boxplot, we can see that treatment group with HighDose will bring higher Born Mineral Density. But if the treatment for mice is LowDose, the Born Mineral Density would be lower than even control group. Therefore, the HighDose may bring positive effect, and LowDose may bring negative effects.

###Question 15 
```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=10, fig.height=12}
# for balanced designs only
print(aov(BMD ~ Treatment, data=kudzu_data))

print(summary(aov(BMD ~ Treatment, data=kudzu_data)))
# looking for normal distribution 
temp <- kudzu_data %>%
  group_by(Treatment) %>%
  summarize(mean(BMD))
left_join(kudzu_data, temp) %>%
  mutate(residuals = BMD - `mean(BMD)`) %>%
  ggplot(aes(sample=residuals)) +
  stat_qq() +
  stat_qq_line() +
  ggtitle("Normal Quantile Plot of the\nOne-Way ANOVA Model Residuals") +
  theme.info

# looking at response by treatment group
kudzu_data %>%
  ggplot(aes(sample=BMD)) +
  facet_grid(~ Treatment) +
  stat_qq() + 
  stat_qq_line() +
  ggtitle("Normal Quantile Plots by Treatment Group") +
  theme.info
```

Assumptions:

1) independent observations : Since sample is randomly selected, assumption Satisfied.

2）balanced design: Satisfied.

3) assume εij are normally distributed with mean 0 and standard deviation σ
i.e.,εij ∼N(0,σ), From above qq-plot, it could be seem that our residules satisifed this assumption.

4）constant variance: rule of thumb about group standard deviations check the variance is constant. Assumption Satisfied.

5) normally distributed measurements in each group with the same population standard deviation, Assumption Satisfied.

###Question 16 
```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=10, fig.height=12}
print(aov(BMD ~ Treatment, data=kudzu_data))
print(summary(aov(BMD ~ Treatment, data=kudzu_data)))
print(oneway.test(kudzu_data$BMD ~ kudzu_data$Treatment, var.equal=TRUE))
```
estimate of error standard deviation −→ s = 0.01436563 in grams per square centimeter (i.e., RMSE)
H0 : μcontrol = μhighdose = μlowdose
Ha : at least two means are different
α = 0.01
test statistic:F = 7.7182, num df = 2, denom df = 42, p-value = 0.001397
p-value= 0.001397 < 0.01 = α −→ reject null hypothesis H0
−→ at least two means are different

###Question 17 
```{r,warning = FALSE,error = FALSE,message = FALSE,fig.width=10, fig.height=10}
print(pairwise.t.test(x=kudzu_data$BMD, g=kudzu_data$Treatment, p.adjust="none"))
print(pairwise.t.test(x=kudzu_data$BMD, g=kudzu_data$Treatment, p.adjust="bonferroni"))

# Tukey's HSD

result <- aov(BMD ~ Treatment, data=kudzu_data)

print(TukeyHSD(result, conf.level=0.95))

par(mar=c(5, 14, 4, 2))
plot(TukeyHSD(result, conf.level=0.95), las=TRUE)+theme.info
```

Tukey’s multiple-comparisons methodH0 : 

H0 : μcontrol = μhighdose

H0 : μhighdose = μlowdose

H0 : μcontrol = μlowdose

From "none" method and "bonferroni" method, HighDose is significantly different from LowDose Group and Control Group.

From Tukey's multiple-comparisons method,significantly different pairs have confidence intervals which do not include 0, which are LowDose-HighDose and HighDose - Control.

